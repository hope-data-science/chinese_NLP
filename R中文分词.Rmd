---
title: "R语言自然语言处理：中文分词"
author: "作者：黄天元，复旦大学博士在读，热爱数据科学与R，热衷推广R在业界的应用。邮箱：huang.tian-yuan@qq.com.欢迎合作交流"
output: word_document
---

  R有很多自然语言处理的包，但是大多是针对英文的。中文来做NLP的包，经过长期探索，认为目前要做中文的NLP，首推jiebaR包。本文主要讲如何对中文进行分词，分词的概念就是把一个句子分成词语。如果在英文中，词语之间都有空格，因此分词非常简单。但是中文都连在一起，因此必须用一定的算法来分开。
  举例：

1. 英文：“R is my favorite programming language.”
2. 中文：“R语言是我最喜爱的编程语言。”

  现在，我们利用jiebaR包对这句话进行分词。
  
## 快速入门
  首先，加载必要的包。
```{r}
library(pacman)
p_load(jiebaR)
```
  然后，我们马上对句子进行分词。
```{r}
en = "R is my favorite programming language."
cn = "R语言是我最喜爱的编程语言"

worker() -> wk
segment(en,wk)
```
  看出来了吧，英文分词根本没有难度，可以直接用空格分开所有组分。下面来看中文：
```{r}
segment(cn,wk)
```
  美中不足的地方是，“R”和“语言”分了开来。
  
## 自定义词典
  不过有的时候，如果没有自定义词典，有的词是无论如何分不对的。打个比方，如果“爸爸去哪儿”是一个词，但是分词的时候一般很难分出来。此外，我们的“R语言”也被分为了两部分。这样分词是不对的，为了让这种情况不再发生，我们必须自定义词典。首先，要看词典在哪里。
```{r}
show_dictpath() 
```
  来到这个路径下，然后对“user.dict.utf8”这个文件进行更改。使用记事本打开，然后在最后补上词条，也就是“R语言”。现在再来进行分词（不过我们要重新定义worker才能更新）：
```{r}
worker() -> wk

segment(cn,wk)
```
  这次R语言已经变成了一个词组。
  事实上，中文一直在变化，想要用一个算法就永远解决分词的问题，是不存在的（我从来没有更改过分词的模式，因为默认的情况已经足够解决大多数问题）。但是我们可以定期更新我们词库，从而让分词的效果能够维持在比较高的水平。
  
## 获取更多的词典
  所以我们知道我们想要更多的词典，得到海量能够跟得上时代发展的词。有什么办法？我认为能够得到大量新词的地方，有两个：1.搜索引擎；2.输入法。搜狗输入法在搜狗词胞库中提供了大量的词典（<https://pinyin.sogou.com/dict/>），大家可以自由下载。不过下载的文件格式是以“.scel”为后缀的，双击一般就给我们的输入法补充了一个词库，但是没法让我们的jiebaR直接利用。幸好jiebaR的作者为我们提供了转格式工具，能够把scel文件直接转化为.utf8格式的文本文件，从而直接对这些词进行利用。具体内容可以参照<https://github.com/qinwf/cidian>，这里给出懒人加载的版本。
```{r,eval=FALSE}
p_load(devtools,stringi,pbapply,Rcpp,RcppProgress)
install_github("qinwf/cidian")
```
  那么，大家就可以愉快地使用一个简单的函数来进行转格式了：
```{r,eval=FALSE}
decode_scel(scel = "细胞词库路径", output = "输出文件路径", cpp =  TRUE)
```
  关于更多个性化的用法，大家可以去官网查询。
  
## 小结
  我认为至此，中文分词已经足够好用。我相信大神永远能够对算法进行革新，从而让分词更加准确。可惜我本人没学习过分词算法，没有办法在算法的层面来做一些事情。但是想办法得到自己想要得到的目标关键词词库，还是相对简单的，这样一来我们已经解决了大部分垂直领域的问题。
  



  
